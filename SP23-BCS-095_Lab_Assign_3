{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMbusHh5ehimYH0aN3m8YRP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Name:** M.Musadiq\n","# **Reg No.** SP23-BCS-095\n","# **Section: C**\n","# **Assignment No. 3**\n","\n","\n"],"metadata":{"id":"-bNLu0b592z3"}},{"cell_type":"markdown","source":["**1. Memory Allocation**\n","\n","1. Allocate two arrays A and B of size N = 1024 on the CPU (host).\n","2. Initialize A[i] = i and B[i] = 2*i.\n","3. Allocate corresponding memory on the GPU using cudaMalloc.\n","4. Copy data from host â†’ device using cudaMemcpy.List item\n","\n","\n"],"metadata":{"id":"4PxiH8vH26pc"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDo9zFkC0sjr","executionInfo":{"status":"ok","timestamp":1759390685813,"user_tz":-300,"elapsed":89,"user":{"displayName":"MUHAMMAD MUSADIQ","userId":"06152962410006965021"}},"outputId":"5d85b76f-444d-46b7-cdb0-b690f6e39190"},"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 values after GPU transfer:\n","A[0] = 0, B[0] = 0\n","A[1] = 1, B[1] = 2\n","A[2] = 2, B[2] = 4\n","A[3] = 3, B[3] = 6\n","A[4] = 4, B[4] = 8\n"]}],"source":["import numpy as np\n","from numba import cuda\n","\n","N = 1024\n","\n","A = np.arange(N, dtype=np.int32)\n","B = np.arange(N, dtype=np.int32) * 2\n","\n","d_A = cuda.to_device(A)\n","d_B = cuda.to_device(B)\n","\n","A_back = d_A.copy_to_host()\n","B_back = d_B.copy_to_host()\n","\n","print(\"First 5 values after GPU transfer:\")\n","for i in range(5):\n","    print(f\"A[{i}] = {A_back[i]}, B[{i}] = {B_back[i]}\")\n"]},{"cell_type":"markdown","source":["**Kernels**\n","1. Write two kernels:\n","    *   kernel1: Computes C[i] = A[i] + B[i].\n","    *   kernel2: Computes D[i] = C[i] * C[i].\n","    \n","2. Launch kernel1 and kernel2 serially on the default stream (stream 0). Ensure results are written back in order."],"metadata":{"id":"-Iu0GR3b3SBw"}},{"cell_type":"code","source":["import cupy as cp\n","\n","kernel1_code = r'''\n","extern \"C\" __global__\n","void kernel1(const int* A, const int* B, int* C, int N) {\n","    int i = blockDim.x * blockIdx.x + threadIdx.x;\n","    if (i < N) {\n","        C[i] = A[i] + B[i];\n","    }\n","}\n","'''\n","\n","kernel2_code = r'''\n","extern \"C\" __global__\n","void kernel2(const int* C, int* D, int N) {\n","    int i = blockDim.x * blockIdx.x + threadIdx.x;\n","    if (i < N) {\n","        D[i] = C[i] * C[i];\n","    }\n","}\n","'''\n","\n","kernel1 = cp.RawKernel(kernel1_code, 'kernel1')\n","kernel2 = cp.RawKernel(kernel2_code, 'kernel2')\n","\n","N = 1024\n","threads_per_block = 128\n","blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n","\n","A = cp.arange(N, dtype=cp.int32)\n","B = cp.arange(N, dtype=cp.int32) * 2\n","C = cp.zeros(N, dtype=cp.int32)\n","D = cp.zeros(N, dtype=cp.int32)\n","\n","kernel1((blocks_per_grid,), (threads_per_block,), (A, B, C, N))\n","kernel2((blocks_per_grid,), (threads_per_block,), (C, D, N))\n","\n","A_host = cp.asnumpy(A)\n","B_host = cp.asnumpy(B)\n","C_host = cp.asnumpy(C)\n","D_host = cp.asnumpy(D)\n","\n","print(\"First 5 results:\")\n","for i in range(5):\n","    print(f\"A[{i}] = {A_host[i]}, B[{i}] = {B_host[i]}, C[{i}] = {C_host[i]}, D[{i}] = {D_host[i]}\")\n"],"metadata":{"id":"oTJv48qK252p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Streams**\n","1. Now modify your code to launch the same two kernels on two different streams (stream1 and stream2) so they may overlap.\n","2. Discuss: What race condition might occur if both kernels write to the same output array? How would you fix it?"],"metadata":{"id":"EEXZVX1z501i"}},{"cell_type":"code","source":["import cupy as cp\n","\n","kernel1_code = r'''\n","extern \"C\" __global__\n","void kernel1(const int* A, const int* B, int* C, int N) {\n","    int i = blockDim.x * blockIdx.x + threadIdx.x;\n","    if (i < N) {\n","        C[i] = A[i] + B[i];\n","    }\n","}\n","''';\n","\n","kernel2_code = r'''\n","extern \"C\" __global__\n","void kernel2(const int* C, int* D, int N) {\n","    int i = blockDim.x * blockIdx.x + threadIdx.x;\n","    if (i < N) {\n","        D[i] = C[i] * C[i];\n","    }\n","}\n","''';\n","\n","kernel1 = cp.RawKernel(kernel1_code, 'kernel1')\n","kernel2 = cp.RawKernel(kernel2_code, 'kernel2')\n","\n","N = 1024\n","threads_per_block = 128\n","blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n","\n","A = cp.arange(N, dtype=cp.int32)\n","B = cp.arange(N, dtype=cp.int32) * 2\n","C = cp.zeros(N, dtype=cp.int32)\n","D = cp.zeros(N, dtype=cp.int32)\n","\n","stream1 = cp.cuda.Stream()\n","stream2 = cp.cuda.Stream()\n","\n","with stream1:\n","    kernel1((blocks_per_grid,), (threads_per_block,), (A, B, C, N))\n","\n","with stream2:\n","    kernel2((blocks_per_grid,), (threads_per_block,), (C, D, N))\n","\n","stream1.synchronize()\n","stream2.synchronize()\n","\n","A_host = cp.asnumpy(A)\n","B_host = cp.asnumpy(B)\n","C_host = cp.asnumpy(C)\n","D_host = cp.asnumpy(D)\n","\n","print(\"\\nFirst 5 results with streams:\")\n","for i in range(5):\n","    print(f\"A[{i}] = {A_host[i]}, B[{i}] = {B_host[i]}, \"\n","          f\"C[{i}] = {C_host[i]}, D[{i}] = {D_host[i]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WjCk9LnD5ylE","executionInfo":{"status":"ok","timestamp":1759391465440,"user_tz":-300,"elapsed":2552,"user":{"displayName":"MUHAMMAD MUSADIQ","userId":"06152962410006965021"}},"outputId":"eb658452-9554-4e4d-84d9-9a64c2b6e1a2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","First 5 results with streams:\n","A[0] = 0, B[0] = 0, C[0] = 0, D[0] = 0\n","A[1] = 1, B[1] = 2, C[1] = 3, D[1] = 9\n","A[2] = 2, B[2] = 4, C[2] = 6, D[2] = 36\n","A[3] = 3, B[3] = 6, C[3] = 9, D[3] = 81\n","A[4] = 4, B[4] = 8, C[4] = 12, D[4] = 144\n"]}]},{"cell_type":"markdown","source":["#   **Problem:**\n","\n","**Race condition.**\n","\n","If both kernels write to the same output array, then one kernel might overwrite what the other is writing. For example:\n","\n","* Kernel1 writes into C.\n","\n","* Kernel2 also tries to write into C.\n","\n","Now, the order isn't guaranteed so results might be corrupted.\n","\n","Fix: Always write to separate arrays (C and D in our case). Or if you must share, add proper synchronization.\n","\n","So streams = overlap = speed, but we must be careful with who writes where."],"metadata":{"id":"7yB5ya7v8xQ5"}},{"cell_type":"markdown","source":["**Synchronization**\n","1. Show the difference between:\n","     \n","    * Calling cudaDeviceSynchronize() after kernel2 before copying results back.\n","2. Not calling cudaDeviceSynchronize() (CPU may print results before GPU\n","finishes)."],"metadata":{"id":"cuJuKoHe6mFu"}},{"cell_type":"code","source":["kernel_slow_code = r'''\n","extern \"C\" __global__\n","void slow_kernel(int* D, int N) {\n","    int i = blockDim.x * blockIdx.x + threadIdx.x;\n","    if (i < N) {\n","        long long temp = 0;\n","        for (int j = 0; j < 100000; ++j) { // artificial delay\n","            temp += j;\n","        }\n","        D[i] = temp;\n","    }\n","}\n","''';\n","\n","slow_kernel = cp.RawKernel(kernel_slow_code, 'slow_kernel')\n","\n","N = 1024 * 128\n","threads_per_block = 128\n","blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n","D = cp.zeros(N, dtype=cp.int32)\n","\n","print(\"\\n--- Case 1: With cudaDeviceSynchronize() ---\")\n","start = time.time()\n","slow_kernel((blocks_per_grid,), (threads_per_block,), (D, N))\n","cp.cuda.Device().synchronize()\n","print(\"Time with sync:\", time.time() - start, \"seconds\")\n","\n","print(\"\\n--- Case 2: Without cudaDeviceSynchronize() ---\")\n","D = cp.zeros(N, dtype=cp.int32)\n","start = time.time()\n","slow_kernel((blocks_per_grid,), (threads_per_block,), (D, N))\n","print(\"Time without sync (CPU doesn't wait):\", time.time() - start, \"seconds\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHeWfbt75-zR","executionInfo":{"status":"ok","timestamp":1759391638094,"user_tz":-300,"elapsed":11,"user":{"displayName":"MUHAMMAD MUSADIQ","userId":"06152962410006965021"}},"outputId":"ba760313-1d1d-4397-c091-916b4f913c7b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Case 1: With cudaDeviceSynchronize() ---\n","Time with sync: 0.05173206329345703 seconds\n","\n","--- Case 2: Without cudaDeviceSynchronize() ---\n","Time without sync (CPU doesn't wait): 0.00012135505676269531 seconds\n"]}]},{"cell_type":"markdown","source":["**Thread Hierarchy**\n","1. First, launch kernel1&lt;&lt;&lt;1, N&gt;&gt;&gt; (1 block, N threads).\n","2. Then, launch kernel1&lt;&lt;&lt;N/32, 32&gt;&gt;&gt; (N/32 blocks, 32 threads per block).\n","3. Compare: How does the thread indexing differ (threadIdx, blockIdx)?"],"metadata":{"id":"KTmY2KDX7DO0"}},{"cell_type":"code","source":["import cupy as cp\n","\n","kernel_info_code = r'''\n","extern \"C\" __global__\n","void kernel_info(int N) {\n","    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (tid < N) {\n","        printf(\"blockIdx=%d, threadIdx=%d, globalId=%d\\n\",\n","               blockIdx.x, threadIdx.x, tid);\n","    }\n","}\n","''';\n","\n","kernel_info = cp.RawKernel(kernel_info_code, 'kernel_info')\n","\n","N = 32  # small for readability\n","\n","print(\"\\n--- Launch with <<<1, N>>> (1 block, N threads) ---\")\n","kernel_info((1,), (N,), (N,))\n","cp.cuda.Device().synchronize()\n","\n","print(\"\\n--- Launch with <<<N/32, 32>>> (N/32 blocks, 32 threads each) ---\")\n","kernel_info((N//32,), (32,), (N,))\n","cp.cuda.Device().synchronize()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POZ_ncpA7Pjm","executionInfo":{"status":"ok","timestamp":1759391858439,"user_tz":-300,"elapsed":13,"user":{"displayName":"MUHAMMAD MUSADIQ","userId":"06152962410006965021"}},"outputId":"71e9c201-718e-495b-ce25-569897e7f9ff"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Launch with <<<1, N>>> (1 block, N threads) ---\n","\n","--- Launch with <<<N/32, 32>>> (N/32 blocks, 32 threads each) ---\n"]}]},{"cell_type":"markdown","source":["**Bonus Challenge**\n","1. Extend your code to compute the sum of all elements in D[] using:\n","    * Shared memory reduction inside a kernel.\n","    * Avoid race conditions using atomicAdd if multiple blocks are used."],"metadata":{"id":"Ani4vmGL7V52"}},{"cell_type":"code","source":["# Shared Memory\n","\n","import cupy as cp\n","\n","reduction_code = r'''\n","extern \"C\" __global__\n","void reduce_shared(const int* D, int* block_sums, int N) {\n","    extern __shared__ int sdata[];\n","\n","    int tid = threadIdx.x;\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    // load into shared memory\n","    sdata[tid] = (i < N) ? D[i] : 0;\n","    __syncthreads();\n","\n","    // do reduction in shared memory\n","    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if (tid < s) {\n","            sdata[tid] += sdata[tid + s];\n","        }\n","        __syncthreads();\n","    }\n","\n","    // first thread writes block result\n","    if (tid == 0) {\n","        block_sums[blockIdx.x] = sdata[0];\n","    }\n","}\n","''';\n","\n","reduce_shared = cp.RawKernel(reduction_code, 'reduce_shared')\n","N = 1024\n","threads_per_block = 128\n","blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n","\n","D = cp.arange(N, dtype=cp.int32)\n","\n","block_sums = cp.zeros(blocks_per_grid, dtype=cp.int32)\n","\n","reduce_shared((blocks_per_grid,), (threads_per_block,),\n","              (D, block_sums, N),\n","              shared_mem=threads_per_block * cp.dtype(cp.int32).itemsize)\n","\n","sum_shared = int(cp.asnumpy(block_sums).sum())\n","print(\"Sum using shared memory reduction =\", sum_shared)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TFjjnva7lyN","executionInfo":{"status":"ok","timestamp":1759392010645,"user_tz":-300,"elapsed":55,"user":{"displayName":"MUHAMMAD MUSADIQ","userId":"06152962410006965021"}},"outputId":"286f41fe-e16a-49d0-f81d-8b5484d8fea2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Sum using shared memory reduction = 523776\n"]}]},{"cell_type":"code","source":["# Atomic Code\n","\n","atomic_code = r'''\n","extern \"C\" __global__\n","void reduce_atomic(const int* D, int* result, int N) {\n","    int i = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (i < N) {\n","        atomicAdd(result, D[i]);\n","    }\n","}\n","''';\n","\n","reduce_atomic = cp.RawKernel(atomic_code, 'reduce_atomic')\n","result = cp.zeros(1, dtype=cp.int32)\n","reduce_atomic((blocks_per_grid,), (threads_per_block,), (D, result, N))\n","sum_atomic = int(cp.asnumpy(result)[0])\n","print(\"Sum using atomicAdd =\", sum_atomic)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5nkbB-T276i8","executionInfo":{"status":"ok","timestamp":1759392034629,"user_tz":-300,"elapsed":48,"user":{"displayName":"MUHAMMAD MUSADIQ","userId":"06152962410006965021"}},"outputId":"a6f8526e-4aaa-4a47-fa39-2a167cb73823"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Sum using atomicAdd = 523776\n"]}]}]}