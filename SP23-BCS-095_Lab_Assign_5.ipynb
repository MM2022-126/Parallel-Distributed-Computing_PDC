{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYbwQPR47sGz68PWwQ6DgO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#  Name: M.Musadiq\n","# Reg No. SP23-BCS-095\n","# Section:C\n","# Lab Assignment: 5\n"],"metadata":{"id":"DJDoNXmwWx3L"}},{"cell_type":"markdown","source":["## Step-by-Step Tasks\n","\n","### Task 1: Vector Initialization\n","- Create a large 1D vector **A** of size **N = 10,000,000** on the root process.  \n","- Initialize it with values `A[i] = i + 1`.\n","\n","### Task 2: Data Distribution\n","- Divide **A** into equal sub-vectors and distribute them among all processes using:\n","  - `MPI_Scatter()`\n","- Each process receives its sub-array `local_A`.\n","\n","### Task 3: Local Computation\n","- Each process computes the partial sum of its sub-vector:\n","  ```c\n","  double local_sum = 0;\n","  for (int i = 0; i < local_size; i++)\n","      local_sum += local_A[i];\n"],"metadata":{"id":"fkfP5UXTXekd"}},{"cell_type":"code","source":["!apt-get update -qq\n","!apt-get install -y mpich > /dev/null\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6cmERuAAScnA","executionInfo":{"status":"ok","timestamp":1760640521453,"user_tz":-300,"elapsed":6826,"user":{"displayName":"MUHAMMAD MUSADIQ","userId":"06152962410006965021"}},"outputId":"e90085d6-9c4f-4fe6-a463-749fe37f6635"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"]}]},{"cell_type":"code","source":["%%writefile distributed_sum_full.c\n","#include <mpi.h>\n","#include <stdio.h>\n","#include <stdlib.h>\n","\n","int main(int argc, char* argv[]) {\n","    int rank, size;\n","    long N = 10000000;\n","    double *A = NULL;\n","    double local_sum = 0.0, global_sum = 0.0, avg_sum = 0.0;\n","    double start_time, end_time, parallel_time, serial_time;\n","\n","    MPI_Init(&argc, &argv);\n","    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n","    MPI_Comm_size(MPI_COMM_WORLD, &size);\n","\n","    long local_n = N / size;\n","    long remainder = N % size;\n","    long *sendcounts = NULL, *displs = NULL;\n","\n","    if (rank == 0) {\n","        sendcounts = (long*)malloc(size * sizeof(long));\n","        displs = (long*)malloc(size * sizeof(long));\n","        long offset = 0;\n","        for (int i = 0; i < size; i++) {\n","            sendcounts[i] = local_n + (i < remainder ? 1 : 0);\n","            displs[i] = offset;\n","            offset += sendcounts[i];\n","        }\n","\n","        A = (double*)malloc(N * sizeof(double));\n","        for (long i = 0; i < N; i++)\n","            A[i] = i + 1;\n","    }\n","\n","    long my_local_n;\n","    if (rank == 0) my_local_n = sendcounts[0];\n","    MPI_Scatter(sendcounts, 1, MPI_LONG, &my_local_n, 1, MPI_LONG, 0, MPI_COMM_WORLD);\n","\n","    double *local_A = (double*)malloc(my_local_n * sizeof(double));\n","\n","    start_time = MPI_Wtime();\n","    MPI_Scatterv(A, sendcounts, displs, MPI_DOUBLE, local_A, my_local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n","\n","    for (long i = 0; i < my_local_n; i++)\n","        local_sum += local_A[i];\n","\n","    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n","    end_time = MPI_Wtime();\n","    parallel_time = end_time - start_time;\n","\n","    MPI_Allreduce(&local_sum, &avg_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n","    avg_sum /= N;\n","\n","    if (rank == 0) {\n","        double expected = (N * (N + 1)) / 2.0;\n","\n","        double serial_sum = 0.0;\n","        double serial_start = MPI_Wtime();\n","        for (long i = 0; i < N; i++)\n","            serial_sum += A[i];\n","        double serial_end = MPI_Wtime();\n","        serial_time = serial_end - serial_start;\n","\n","        printf(\"\\n===== Distributed Partial Summation using MPI =====\\n\");\n","        printf(\"Vector Size (N): %ld\\n\", N);\n","        printf(\"Processes Used: %d\\n\", size);\n","        printf(\"Computed Total Sum = %.0f\\n\", global_sum);\n","        printf(\"Expected Total Sum = %.0f\\n\", expected);\n","        printf(\"Difference = %.5f\\n\", expected - global_sum);\n","        printf(\"Average (MPI_Allreduce) = %.5f\\n\", avg_sum);\n","        printf(\"Parallel Execution Time: %.6f seconds\\n\", parallel_time);\n","        printf(\"Serial Execution Time: %.6f seconds\\n\", serial_time);\n","        printf(\"Speedup = %.2fx\\n\", serial_time / parallel_time);\n","        printf(\"====================================================\\n\");\n","\n","        free(A);\n","        free(sendcounts);\n","        free(displs);\n","    }\n","\n","    free(local_A);\n","    MPI_Finalize();\n","    return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQFCi632Sbfe","executionInfo":{"status":"ok","timestamp":1760640490777,"user_tz":-300,"elapsed":81,"user":{"displayName":"MUHAMMAD MUSADIQ","userId":"06152962410006965021"}},"outputId":"2473a2b6-c856-43a2-8769-1c5e9d198bfa"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing distributed_sum_full.c\n"]}]},{"cell_type":"code","source":["!mpicc distributed_sum.c -o distributed_sum\n"],"metadata":{"id":"eh8E7UQ9SyTA","executionInfo":{"status":"ok","timestamp":1760640493270,"user_tz":-300,"elapsed":275,"user":{"displayName":"MUHAMMAD MUSADIQ","userId":"06152962410006965021"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["!mpirun --allow-run-as-root --oversubscribe -np 4 ./distributed_sum\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2mKv-ECeS11h","executionInfo":{"status":"ok","timestamp":1760640497365,"user_tz":-300,"elapsed":901,"user":{"displayName":"MUHAMMAD MUSADIQ","userId":"06152962410006965021"}},"outputId":"cd5d9662-d285-445e-b403-7ded7a7f49d1"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== Distributed Partial Summation using MPI =====\n","Vector Size (N): 10000000\n","Processes Used: 4\n","Computed Total Sum: 50000005000000\n","Expected Total Sum: 50000005000000\n","Difference: 0.00000\n","Average: 5000000.50000\n","Execution Time: 0.101384 seconds\n","====================================================\n"]}]},{"cell_type":"markdown","source":["### Discussion Questions and Answers\n","\n","1. What happens if N is not divisible by the number of processes?  \n","If N is not evenly divisible, some processes would receive fewer elements, and some data might be skipped. This program fixes that by using variable send counts in MPI_Scatterv.\n","\n","2. How can you modify the program to handle uneven partitions?  \n","Compute arrays of send counts and displacements (sendcounts[] and displs[]) and use MPI_Scatterv instead of MPI_Scatter to ensure each process receives the correct number of elements.\n","\n","3. How would performance differ between using MPI_Reduce versus MPI_Gather plus local summation?  \n","MPI_Reduce performs a distributed sum within the MPI library, which is faster and more memory efficient. MPI_Gather collects all results at the root and sums them serially, which is slower and requires more memory.\n","\n","4. How could this same approach be extended to matrix summation or averaging?  \n","Treat each row or block of rows as a subarray, distribute them with MPI_Scatterv, compute local partial sums, and then use MPI_Reduce or MPI_Allreduce for the total or average.\n","\n","### Bonus Challenge\n","This program already:\n","- Computes both sum and average using a single collective operation (MPI_Allreduce).  \n","- Measures execution time with MPI_Wtime.  \n","- Compares parallel and serial CPU computation to show speedup.\n"],"metadata":{"id":"GRDwmaAMVZoy"}}]}